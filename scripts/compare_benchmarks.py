#!/usr/bin/env python3
"""
Benchmark Comparison Script for ActorScheduler Refactoring

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç¾åœ¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’æ¯”è¼ƒã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’æ¤œå‡ºã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python3 scripts/compare_benchmarks.py \\
        --baseline benchmarks/baseline_before_refactor.md \\
        --current target/criterion/

å‡ºåŠ›:
    - æ¨™æº–å‡ºåŠ›: ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¯”è¼ƒçµæœ
    - benchmark_comparison.md: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ
    - çµ‚äº†ã‚³ãƒ¼ãƒ‰: 0=æ­£å¸¸, 1=å›å¸°æ¤œå‡º, 2=ã‚¨ãƒ©ãƒ¼
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import re


class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""

    def __init__(self, name: str, mean: float, std_dev: float, unit: str = "ns"):
        self.name = name
        self.mean = mean
        self.std_dev = std_dev
        self.unit = unit

    def __repr__(self):
        return f"{self.name}: {self.mean:.2f} Â±{self.std_dev:.2f} {self.unit}"


def parse_criterion_json(criterion_dir: Path) -> Dict[str, BenchmarkResult]:
    """
    Criterion å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ

    Args:
        criterion_dir: target/criterion/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹

    Returns:
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã‚’ã‚­ãƒ¼ã¨ã—ãŸçµæœã®è¾æ›¸
    """
    results = {}

    # ã™ã¹ã¦ã® benchmark.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
    for json_file in criterion_dir.rglob("new/estimates.json"):
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã‚’æŠ½å‡ºï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰ï¼‰
            # ä¾‹: target/criterion/mailbox_throughput/bounded_1000/new/estimates.json
            #     -> mailbox_throughput/bounded_1000
            # criterion/ ã®æ¬¡ã‹ã‚‰ new/ ã®å‰ã¾ã§ã‚’çµåˆ
            parts = json_file.parts
            try:
                criterion_idx = parts.index("criterion")
                new_idx = parts.index("new")
                benchmark_name = "/".join(parts[criterion_idx + 1:new_idx])
            except (ValueError, IndexError):
                print(f"Warning: Could not parse benchmark name from {json_file}", file=sys.stderr)
                continue

            mean = data.get("mean", {}).get("point_estimate", 0)
            std_dev = data.get("std_dev", {}).get("point_estimate", 0)

            results[benchmark_name] = BenchmarkResult(
                name=benchmark_name, mean=mean, std_dev=std_dev, unit="ns"
            )

        except (json.JSONDecodeError, KeyError) as e:
            print(f"Warning: Failed to parse {json_file}: {e}", file=sys.stderr)
            continue

    return results


def parse_baseline_markdown(baseline_file: Path) -> Dict[str, BenchmarkResult]:
    """
    ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’è§£æ

    ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹:
    | Benchmark | Mean | Std Dev |
    |-----------|------|---------|
    | mailbox_throughput/bounded_1000 | 1234.56 ns | 12.34 ns |

    Args:
        baseline_file: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã‚’ã‚­ãƒ¼ã¨ã—ãŸçµæœã®è¾æ›¸
    """
    results = {}

    with open(baseline_file, "r") as f:
        content = f.read()

    # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‚’æŠ½å‡ºï¼ˆ|ã§å§‹ã¾ã‚Š|ã§çµ‚ã‚ã‚‹è¡Œï¼‰
    table_pattern = r'\|\s*(.+?)\s*\|\s*([\d.]+)\s*ns\s*\|\s*([\d.]+)\s*ns\s*\|'

    for match in re.finditer(table_pattern, content):
        name = match.group(1).strip()
        if name == "Benchmark" or name.startswith("-"):
            continue  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—

        mean = float(match.group(2))
        std_dev = float(match.group(3))

        results[name] = BenchmarkResult(name=name, mean=mean, std_dev=std_dev)

    return results


def compare_results(
    baseline: Dict[str, BenchmarkResult], current: Dict[str, BenchmarkResult]
) -> List[Tuple[str, float, bool]]:
    """
    ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç¾åœ¨ã®çµæœã‚’æ¯”è¼ƒ

    Args:
        baseline: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœ
        current: ç¾åœ¨ã®çµæœ

    Returns:
        (ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å, å¤‰åŒ–ç‡(%), å›å¸°ãƒ•ãƒ©ã‚°) ã®ãƒªã‚¹ãƒˆ
    """
    comparisons = []

    for name in sorted(set(baseline.keys()) | set(current.keys())):
        if name not in baseline:
            # æ–°è¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            comparisons.append((name, None, False))
            continue

        if name not in current:
            # å‰Šé™¤ã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            comparisons.append((name, None, True))
            continue

        baseline_mean = baseline[name].mean
        current_mean = current[name].mean

        # å¤‰åŒ–ç‡ã‚’è¨ˆç®—ï¼ˆæ­£: æ‚ªåŒ–, è² : æ”¹å–„ï¼‰
        if baseline_mean > 0:
            change_percent = ((current_mean - baseline_mean) / baseline_mean) * 100
        else:
            change_percent = 0

        # 5% ä»¥ä¸Šã®æ‚ªåŒ–ã‚’å›å¸°ã¨åˆ¤å®š
        is_regression = change_percent > 5.0

        comparisons.append((name, change_percent, is_regression))

    return comparisons


def generate_report(
    comparisons: List[Tuple[str, float, bool]], output_file: Path
) -> None:
    """
    æ¯”è¼ƒçµæœã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ç”Ÿæˆ

    Args:
        comparisons: compare_results() ã®å‡ºåŠ›
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    with open(output_file, "w") as f:
        f.write("# Benchmark Comparison Report\n\n")
        f.write("## Summary\n\n")

        total_count = len(comparisons)
        regression_count = sum(1 for _, _, is_reg in comparisons if is_reg)
        improvement_count = sum(
            1 for _, change, _ in comparisons if change is not None and change < -5.0
        )

        f.write(f"- **Total Benchmarks**: {total_count}\n")
        f.write(f"- **Regressions (>5% slower)**: {regression_count}\n")
        f.write(f"- **Improvements (>5% faster)**: {improvement_count}\n\n")

        f.write("## Detailed Results\n\n")
        f.write("| Benchmark | Change | Status |\n")
        f.write("|-----------|--------|--------|\n")

        for name, change, is_regression in sorted(
            comparisons, key=lambda x: x[1] if x[1] is not None else 0, reverse=True
        ):
            if change is None:
                status = "âš ï¸ New or Removed"
                change_str = "N/A"
            elif is_regression:
                status = "ğŸ”´ Regression"
                change_str = f"+{change:.2f}%"
            elif change < -5.0:
                status = "ğŸŸ¢ Improvement"
                change_str = f"{change:.2f}%"
            else:
                status = "âšª No Change"
                change_str = f"{change:+.2f}%"

            f.write(f"| {name} | {change_str} | {status} |\n")

        f.write("\n---\n")
        f.write("Generated by scripts/compare_benchmarks.py\n")


def print_summary(comparisons: List[Tuple[str, float, bool]]) -> None:
    """
    æ¯”è¼ƒçµæœã®ã‚µãƒãƒªã‚’æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤º

    Args:
        comparisons: compare_results() ã®å‡ºåŠ›
    """
    print("\n" + "=" * 70)
    print("BENCHMARK COMPARISON SUMMARY")
    print("=" * 70 + "\n")

    regressions = [
        (name, change) for name, change, is_reg in comparisons
        if is_reg and change is not None
    ]

    if regressions:
        print("âš ï¸  REGRESSIONS DETECTED (>5% slower):\n")
        for name, change in sorted(regressions, key=lambda x: x[1], reverse=True):
            print(f"  ğŸ”´ {name}: +{change:.2f}%")
        print()
    else:
        print("âœ… No significant regressions detected.\n")

    improvements = [
        (name, change)
        for name, change, _ in comparisons
        if change is not None and change < -5.0
    ]

    if improvements:
        print("IMPROVEMENTS (>5% faster):\n")
        for name, change in sorted(improvements, key=lambda x: x[1]):
            print(f"  ğŸŸ¢ {name}: {change:.2f}%")
        print()

    print("=" * 70 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="Compare benchmark results and detect regressions"
    )
    parser.add_argument(
        "--baseline",
        type=Path,
        required=True,
        help="Path to baseline markdown file (e.g., benchmarks/baseline_before_refactor.md)",
    )
    parser.add_argument(
        "--current",
        type=Path,
        required=True,
        help="Path to current criterion output directory (e.g., target/criterion/)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("benchmark_comparison.md"),
        help="Output report file (default: benchmark_comparison.md)",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=5.0,
        help="Regression threshold percentage (default: 5.0)",
    )

    args = parser.parse_args()

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è§£æ
    if not args.baseline.exists():
        print(f"Error: Baseline file not found: {args.baseline}", file=sys.stderr)
        return 2

    print(f"Parsing baseline: {args.baseline}")
    baseline_results = parse_baseline_markdown(args.baseline)
    print(f"  Found {len(baseline_results)} baseline benchmarks")

    # ç¾åœ¨ã®çµæœè§£æ
    if not args.current.exists():
        print(f"Error: Current results directory not found: {args.current}", file=sys.stderr)
        return 2

    print(f"Parsing current results: {args.current}")
    current_results = parse_criterion_json(args.current)
    print(f"  Found {len(current_results)} current benchmarks")

    # æ¯”è¼ƒ
    comparisons = compare_results(baseline_results, current_results)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_report(comparisons, args.output)
    print(f"\nReport generated: {args.output}")

    # ã‚µãƒãƒªè¡¨ç¤º
    print_summary(comparisons)

    # å›å¸°æ¤œå‡ºæ™‚ã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã§çµ‚äº†
    has_regression = any(is_reg for _, _, is_reg in comparisons)
    return 1 if has_regression else 0


if __name__ == "__main__":
    sys.exit(main())
